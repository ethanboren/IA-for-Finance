{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2d1a9c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "\n",
    "# Aesthetic Configuration\n",
    "sns.set_style(\"whitegrid\")\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# ==========================================\n",
    "# 1. DATA LOADING & STATIONARITY CHECK\n",
    "# ==========================================\n",
    "print(\"--- STEP 1: Data Loading ---\")\n",
    "filename = \"sp500_dataset.csv\"\n",
    "df = pd.read_csv(filename, index_col=0, parse_dates=True)\n",
    "\n",
    "# Stationarity Test (Rubric 3.2.1)\n",
    "# We must verify that the Log Returns are stationary (constant mean/variance)\n",
    "# otherwise the model will fail to generalize.\n",
    "print(\"\\nðŸ“Š ADF Test (Augmented Dickey-Fuller) on Log Returns:\")\n",
    "# Note: Ensure the column name matches scraper.py ('Log_Returns')\n",
    "# If using an old CSV, it might be 'Log_Ret'. We check for both for safety or assume new CSV.\n",
    "# We will assume the user runs scraper.py first, so we use 'Log_Returns'.\n",
    "target_col = 'Log_Returns' if 'Log_Returns' in df.columns else 'Log_Ret'\n",
    "vol_col = 'Rolling_Volatility_20' if 'Rolling_Volatility_20' in df.columns else 'Vol_20'\n",
    "ma_col = 'Moving_Average_10' if 'Moving_Average_10' in df.columns else 'MA_10'\n",
    "\n",
    "adf_result = adfuller(df[target_col])\n",
    "print(f\"   p-value: {adf_result[1]:.4e}\")\n",
    "if adf_result[1] < 0.05:\n",
    "    print(\"   âœ… Series is Stationary (Safe to use for modeling).\")\n",
    "else:\n",
    "    print(\"   âš ï¸ Warning: Series is Non-Stationary.\")\n",
    "\n",
    "# ==========================================\n",
    "# 2. PREPARATION (SPLIT & SCALE)\n",
    "# ==========================================\n",
    "print(\"\\n--- STEP 2: Preparation (No Look-Ahead Bias) ---\")\n",
    "\n",
    "# Explanatory Variables (Features)\n",
    "features = ['Lag_1', 'Lag_5', 'Lag_10', vol_col, ma_col, 'VIX', 'RSI']\n",
    "X = df[features].values\n",
    "y = df[['Target']].values \n",
    "\n",
    "# Time-Series Split (80% Train, 20% Test)\n",
    "# CRITICAL: We do NOT shuffle the data. We must respect the chronological order\n",
    "# to avoid \"Look-Ahead Bias\" (training on future data).\n",
    "split = int(len(df) * 0.80)\n",
    "X_train_raw, X_test_raw = X[:split], X[split:]\n",
    "y_train_raw, y_test_raw = y[:split], y[split:]\n",
    "test_dates = df.index[split:]\n",
    "\n",
    "print(f\"   Train set: {len(X_train_raw)} days | Test set: {len(X_test_raw)} days\")\n",
    "\n",
    "# Normalization (MinMax between -1 and 1 for LSTM tanh activation)\n",
    "scaler_X = MinMaxScaler(feature_range=(-1, 1))\n",
    "scaler_y = MinMaxScaler(feature_range=(-1, 1))\n",
    "\n",
    "# Fit ONLY on the Training set! (Rubric 3.2.2)\n",
    "# If we fit on the whole dataset, we leak information from Test to Train.\n",
    "X_train_scaled = scaler_X.fit_transform(X_train_raw)\n",
    "X_test_scaled = scaler_X.transform(X_test_raw)\n",
    "\n",
    "y_train_scaled = scaler_y.fit_transform(y_train_raw)\n",
    "y_test_scaled = scaler_y.transform(y_test_raw)\n",
    "\n",
    "# Reshape for LSTM [Samples, Time Steps, Features]\n",
    "# We use a time step of 1 day for simplicity and interpretability.\n",
    "X_train = X_train_scaled.reshape((X_train_scaled.shape[0], 1, X_train_scaled.shape[1]))\n",
    "X_test = X_test_scaled.reshape((X_test_scaled.shape[0], 1, X_test_scaled.shape[1]))\n",
    "\n",
    "# ==========================================\n",
    "# 3. LSTM MODELING\n",
    "# ==========================================\n",
    "print(\"\\n--- STEP 3: LSTM Architecture ---\")\n",
    "# Simple but effective architecture\n",
    "model = Sequential([\n",
    "    Input(shape=(1, len(features))),\n",
    "    LSTM(50, activation='tanh', return_sequences=False), # Memory layer\n",
    "    Dropout(0.2), # Regularization to prevent overfitting\n",
    "    Dense(1) # Linear output (predicting the return)\n",
    "])\n",
    "\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss='mse')\n",
    "\n",
    "# Training with Early Stopping\n",
    "history = model.fit(\n",
    "    X_train, y_train_scaled,\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    validation_data=(X_test, y_test_scaled),\n",
    "    callbacks=[EarlyStopping(monitor='val_loss', patience=8, restore_best_weights=True)],\n",
    "    verbose=1,\n",
    "    shuffle=False # IMPORTANT: Keep order in batches too (though less critical than split)\n",
    ")\n",
    "\n",
    "# ==========================================\n",
    "# 4. RESULTS & BACKTESTING\n",
    "# ==========================================\n",
    "print(\"\\n--- STEP 4: Financial Evaluation ---\")\n",
    "\n",
    "# Predictions\n",
    "pred_scaled = model.predict(X_test)\n",
    "pred_real = scaler_y.inverse_transform(pred_scaled).flatten()\n",
    "y_test_real = scaler_y.inverse_transform(y_test_scaled).flatten()\n",
    "\n",
    "# Metric 1: MSE (Mean Squared Error)\n",
    "mse = mean_squared_error(y_test_real, pred_real)\n",
    "print(f\"ðŸ“‰ MSE: {mse:.6f}\")\n",
    "\n",
    "# Metric 2: Direction Accuracy\n",
    "# Do we predict the correct sign? (+/-)\n",
    "pred_sign = np.sign(pred_real)\n",
    "true_sign = np.sign(y_test_real)\n",
    "acc = accuracy_score(true_sign, pred_sign)\n",
    "print(f\"ðŸŽ¯ Direction Accuracy: {acc:.2%}\")\n",
    "\n",
    "# Metric 3: Trading Strategy & Sharpe Ratio\n",
    "# Strategy: If Prediction > 0 -> Buy (Long), Else -> Cash (0)\n",
    "# (Simple Long-Only strategy, no short selling)\n",
    "signal = np.where(pred_real > 0, 1, 0)\n",
    "strategy_ret = signal * y_test_real\n",
    "\n",
    "# Buy & Hold (Benchmark)\n",
    "cum_market = (1 + y_test_real).cumprod()\n",
    "cum_strategy = (1 + strategy_ret).cumprod()\n",
    "\n",
    "# Sharpe Ratio Calculation (Annualized)\n",
    "# Assuming Risk-Free Rate ~ 4% (0.04/252 daily)\n",
    "rf = 0.04 / 252\n",
    "excess_ret = strategy_ret - rf\n",
    "sharpe = np.sqrt(252) * np.mean(excess_ret) / (np.std(excess_ret) + 1e-9)\n",
    "print(f\"ðŸ’° Sharpe Ratio (Strategy): {sharpe:.2f}\")\n",
    "print(f\"ðŸ“ˆ Total Return Strategy: {(cum_strategy[-1] - 1):.2%}\")\n",
    "print(f\"ðŸ“Š Total Return Market  : {(cum_market[-1] - 1):.2%}\")\n",
    "\n",
    "# Final Plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(test_dates, cum_market, label='Benchmark (Buy & Hold)', color='gray', alpha=0.6)\n",
    "plt.plot(test_dates, cum_strategy, label=f'LSTM Strategy (Sharpe: {sharpe:.2f})', color='green')\n",
    "plt.title(f'Performance: LSTM vs S&P 500 ({test_dates[0].year}-{test_dates[-1].year})')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Cumulative Return')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (MPS)",
   "language": "python",
   "name": "mps-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
