{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c0215ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import pandas_ta as ta\n",
    "import numpy as np\n",
    "import datetime\n",
    "\n",
    "# ==========================================\n",
    "# 1. CONFIGURATION & DOWNLOAD\n",
    "# ==========================================\n",
    "# We fetch a long history (from 2000) to ensure sufficient data for training\n",
    "start_date = \"2000-01-01\"\n",
    "end_date = (datetime.date.today() + datetime.timedelta(days=1)).strftime(\"%Y-%m-%d\")\n",
    "filename = \"sp500_dataset.csv\"\n",
    "\n",
    "print(f\"üì• Downloading S&P 500 and VIX data ({start_date} to {end_date})...\")\n",
    "\n",
    "# S&P 500 (^GSPC)\n",
    "sp500 = yf.download(\"^GSPC\", start=start_date, end=end_date, auto_adjust=True)\n",
    "if isinstance(sp500.columns, pd.MultiIndex):\n",
    "    sp500.columns = sp500.columns.droplevel(1)\n",
    "\n",
    "# VIX (^VIX) - Exogenous Variable (Market Fear Index)\n",
    "vix = yf.download(\"^VIX\", start=start_date, end=end_date, auto_adjust=True)\n",
    "if isinstance(vix.columns, pd.MultiIndex):\n",
    "    vix.columns = vix.columns.droplevel(1)\n",
    "\n",
    "# Merge on Date index\n",
    "df = pd.merge(sp500[['Close', 'Volume']], vix[['Close']], \n",
    "              left_index=True, right_index=True, suffixes=('', '_VIX'), how='inner')\n",
    "df.rename(columns={'Close_VIX': 'VIX'}, inplace=True)\n",
    "\n",
    "# ==========================================\n",
    "# 2. FEATURE ENGINEERING\n",
    "# ==========================================\n",
    "print(\"‚öôÔ∏è Calculating financial indicators...\")\n",
    "\n",
    "# A. Log Returns (Essential for Stationarity)\n",
    "# Formula: R_t = ln(P_t / P_{t-1})\n",
    "# We use Log Returns instead of prices because prices are non-stationary (trends).\n",
    "df['Log_Returns'] = np.log(df['Close'] / df['Close'].shift(1))\n",
    "\n",
    "# B. Lagged Returns (1, 5, 10 days)\n",
    "# Past returns used as features to predict future returns.\n",
    "df['Lag_1'] = df['Log_Returns'].shift(1)\n",
    "df['Lag_5'] = df['Log_Returns'].shift(5)\n",
    "df['Lag_10'] = df['Log_Returns'].shift(10)\n",
    "\n",
    "# C. Rolling Volatility (20-day Risk)\n",
    "# Standard deviation of returns over the past month (~20 trading days).\n",
    "df['Rolling_Volatility_20'] = df['Log_Returns'].rolling(window=20).std()\n",
    "\n",
    "# D. Moving Averages (Trend)\n",
    "# Simple Moving Average of returns to capture short-term momentum.\n",
    "df['Moving_Average_10'] = df['Log_Returns'].rolling(window=10).mean()\n",
    "\n",
    "# E. RSI (Relative Strength Index)\n",
    "# Technical momentum indicator.\n",
    "df['RSI'] = ta.rsi(df['Close'], length=14)\n",
    "\n",
    "# F. TARGET VARIABLE: Next Day's Return\n",
    "# We shift the Log Returns backwards by 1 to align today's features with tomorrow's return.\n",
    "df['Target'] = df['Log_Returns'].shift(-1)\n",
    "\n",
    "# ==========================================\n",
    "# 3. CLEANUP & SAVE\n",
    "# ==========================================\n",
    "# Remove NaN values created by lags and rolling windows\n",
    "df_clean = df.dropna()\n",
    "\n",
    "# Keep only relevant columns for the model\n",
    "cols_to_keep = ['Close', 'Log_Returns', 'Target', 'Lag_1', 'Lag_5', 'Lag_10', \n",
    "                'Rolling_Volatility_20', 'Moving_Average_10', 'VIX', 'RSI']\n",
    "df_final = df_clean[cols_to_keep]\n",
    "\n",
    "df_final.to_csv(filename)\n",
    "\n",
    "print(f\"\\n‚úÖ Done! Dataset saved to: {filename}\")\n",
    "print(f\"üìä Final Dimensions: {df_final.shape[0]} trading days.\")\n",
    "print(f\"üìå Columns: {df_final.columns.tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f221d1d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "\n",
    "# Aesthetic Configuration\n",
    "sns.set_style(\"whitegrid\")\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# ==========================================\n",
    "# 1. DATA LOADING & STATIONARITY CHECK\n",
    "# ==========================================\n",
    "print(\"--- STEP 1: Data Loading ---\")\n",
    "filename = \"sp500_dataset.csv\"\n",
    "df = pd.read_csv(filename, index_col=0, parse_dates=True)\n",
    "\n",
    "# Stationarity Test (Rubric 3.2.1)\n",
    "# We must verify that the Log Returns are stationary (constant mean/variance)\n",
    "# otherwise the model will fail to generalize.\n",
    "print(\"\\nüìä ADF Test (Augmented Dickey-Fuller) on Log Returns:\")\n",
    "# Note: Ensure the column name matches scraper.py ('Log_Returns')\n",
    "# If using an old CSV, it might be 'Log_Ret'. We check for both for safety or assume new CSV.\n",
    "# We will assume the user runs scraper.py first, so we use 'Log_Returns'.\n",
    "target_col = 'Log_Returns' if 'Log_Returns' in df.columns else 'Log_Ret'\n",
    "vol_col = 'Rolling_Volatility_20' if 'Rolling_Volatility_20' in df.columns else 'Vol_20'\n",
    "ma_col = 'Moving_Average_10' if 'Moving_Average_10' in df.columns else 'MA_10'\n",
    "\n",
    "adf_result = adfuller(df[target_col])\n",
    "print(f\"   p-value: {adf_result[1]:.4e}\")\n",
    "if adf_result[1] < 0.05:\n",
    "    print(\"   ‚úÖ Series is Stationary (Safe to use for modeling).\")\n",
    "else:\n",
    "    print(\"   ‚ö†Ô∏è Warning: Series is Non-Stationary.\")\n",
    "\n",
    "# ==========================================\n",
    "# 2. PREPARATION (SPLIT & SCALE)\n",
    "# ==========================================\n",
    "print(\"\\n--- STEP 2: Preparation (No Look-Ahead Bias) ---\")\n",
    "\n",
    "# Explanatory Variables (Features)\n",
    "features = ['Lag_1', 'Lag_5', 'Lag_10', vol_col, ma_col, 'VIX', 'RSI']\n",
    "X = df[features].values\n",
    "y = df[['Target']].values \n",
    "\n",
    "# Time-Series Split (80% Train, 20% Test)\n",
    "# CRITICAL: We do NOT shuffle the data. We must respect the chronological order\n",
    "# to avoid \"Look-Ahead Bias\" (training on future data).\n",
    "split = int(len(df) * 0.80)\n",
    "X_train_raw, X_test_raw = X[:split], X[split:]\n",
    "y_train_raw, y_test_raw = y[:split], y[split:]\n",
    "test_dates = df.index[split:]\n",
    "\n",
    "print(f\"   Train set: {len(X_train_raw)} days | Test set: {len(X_test_raw)} days\")\n",
    "\n",
    "# Normalization (MinMax between -1 and 1 for LSTM tanh activation)\n",
    "scaler_X = MinMaxScaler(feature_range=(-1, 1))\n",
    "scaler_y = MinMaxScaler(feature_range=(-1, 1))\n",
    "\n",
    "# Fit ONLY on the Training set! (Rubric 3.2.2)\n",
    "# If we fit on the whole dataset, we leak information from Test to Train.\n",
    "X_train_scaled = scaler_X.fit_transform(X_train_raw)\n",
    "X_test_scaled = scaler_X.transform(X_test_raw)\n",
    "\n",
    "y_train_scaled = scaler_y.fit_transform(y_train_raw)\n",
    "y_test_scaled = scaler_y.transform(y_test_raw)\n",
    "\n",
    "# Reshape for LSTM [Samples, Time Steps, Features]\n",
    "# We use a time step of 1 day for simplicity and interpretability.\n",
    "X_train = X_train_scaled.reshape((X_train_scaled.shape[0], 1, X_train_scaled.shape[1]))\n",
    "X_test = X_test_scaled.reshape((X_test_scaled.shape[0], 1, X_test_scaled.shape[1]))\n",
    "\n",
    "# ==========================================\n",
    "# 3. LSTM MODELING\n",
    "# ==========================================\n",
    "print(\"\\n--- STEP 3: LSTM Architecture ---\")\n",
    "# Simple but effective architecture\n",
    "model = Sequential([\n",
    "    Input(shape=(1, len(features))),\n",
    "    LSTM(50, activation='tanh', return_sequences=False), # Memory layer\n",
    "    Dropout(0.2), # Regularization to prevent overfitting\n",
    "    Dense(1) # Linear output (predicting the return)\n",
    "])\n",
    "\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss='mse')\n",
    "\n",
    "# Training with Early Stopping\n",
    "history = model.fit(\n",
    "    X_train, y_train_scaled,\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    validation_data=(X_test, y_test_scaled),\n",
    "    callbacks=[EarlyStopping(monitor='val_loss', patience=8, restore_best_weights=True)],\n",
    "    verbose=1,\n",
    "    shuffle=False # IMPORTANT: Keep order in batches too (though less critical than split)\n",
    ")\n",
    "\n",
    "# ==========================================\n",
    "# 4. RESULTS & BACKTESTING\n",
    "# ==========================================\n",
    "print(\"\\n--- STEP 4: Financial Evaluation ---\")\n",
    "\n",
    "# Predictions\n",
    "pred_scaled = model.predict(X_test)\n",
    "pred_real = scaler_y.inverse_transform(pred_scaled).flatten()\n",
    "y_test_real = scaler_y.inverse_transform(y_test_scaled).flatten()\n",
    "\n",
    "# Metric 1: MSE (Mean Squared Error)\n",
    "mse = mean_squared_error(y_test_real, pred_real)\n",
    "print(f\"üìâ MSE: {mse:.6f}\")\n",
    "\n",
    "# Metric 2: Direction Accuracy\n",
    "# Do we predict the correct sign? (+/-)\n",
    "pred_sign = np.sign(pred_real)\n",
    "true_sign = np.sign(y_test_real)\n",
    "acc = accuracy_score(true_sign, pred_sign)\n",
    "print(f\"üéØ Direction Accuracy: {acc:.2%}\")\n",
    "\n",
    "# Metric 3: Trading Strategy & Sharpe Ratio\n",
    "# Strategy: If Prediction > 0 -> Buy (Long), Else -> Cash (0)\n",
    "# (Simple Long-Only strategy, no short selling)\n",
    "signal = np.where(pred_real > 0, 1, 0)\n",
    "strategy_ret = signal * y_test_real\n",
    "\n",
    "# Buy & Hold (Benchmark)\n",
    "cum_market = (1 + y_test_real).cumprod()\n",
    "cum_strategy = (1 + strategy_ret).cumprod()\n",
    "\n",
    "# Sharpe Ratio Calculation (Annualized)\n",
    "# Assuming Risk-Free Rate ~ 4% (0.04/252 daily)\n",
    "rf = 0.04 / 252\n",
    "excess_ret = strategy_ret - rf\n",
    "sharpe = np.sqrt(252) * np.mean(excess_ret) / (np.std(excess_ret) + 1e-9)\n",
    "print(f\"üí∞ Sharpe Ratio (Strategy): {sharpe:.2f}\")\n",
    "print(f\"üìà Total Return Strategy: {(cum_strategy[-1] - 1):.2%}\")\n",
    "print(f\"üìä Total Return Market  : {(cum_market[-1] - 1):.2%}\")\n",
    "\n",
    "# Final Plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(test_dates, cum_market, label='Benchmark (Buy & Hold)', color='gray', alpha=0.6)\n",
    "plt.plot(test_dates, cum_strategy, label=f'LSTM Strategy (Sharpe: {sharpe:.2f})', color='green')\n",
    "plt.title(f'Performance: LSTM vs S&P 500 ({test_dates[0].year}-{test_dates[-1].year})')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Cumulative Return')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (MPS)",
   "language": "python",
   "name": "mps-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
